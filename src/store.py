from supabase import create_client
from dotenv import load_dotenv
from datetime import timedelta
import chromadb
import hashlib
import os

load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# CLIENT = chromadb.PersistentClient(path=DB_PATH)
CHROMA_KEY = os.getenv("CHROMA_KEY")
client = chromadb.CloudClient(
  api_key=CHROMA_KEY,
  tenant='2b20ce89-8d98-4de1-8644-51260fb2110c',
  database='Test'
)
COLLECTION = client.get_or_create_collection(name="interactions")

def stable_id(date: str, time: str, question: str) -> str:
    q_hash = hashlib.md5(question.encode("utf-8")).hexdigest()
    return f"{date}_{time}_{q_hash}"

def update_db_interactions(data):
    """Insert interactions into Supabase and Chroma Cloud."""
    date = data["date"]
    for log in data["logs"]:
        Q = log["question"]
        A = log["answer"]
        S = log["match_score"]
        T = log["time"]
        E = log["embedding"]

        # 1️⃣ Supabase insert (Relational DB)
        try:
            supabase.table("interactions").insert({
                "date": date,
                "time": T,
                "question": Q,
                "answer": A,
                "match_score": S,
                "embedding": E
            }).execute()
        except Exception as e:
            print(f"⚠️ Duplicate or error: {Q[:30]}... {e}")

        # 2️⃣ Chroma Cloud (Vector DB)
        COLLECTION.add(
            documents=[Q],
            metadatas=[{
                "answer": A,
                "match_score": S,
                "date": date,
                "time": T
            }],
            ids=[stable_id(date, T, Q)],
            embeddings=[E]
            )
        
    print(f"✅ Stored {len(data['logs'])} questions in both Relational and Vector DB for {data['date']}")
    return

def update_db_reports(data, report_text, report_type="daily_reports"):
    """
    Save the generated daily report into the Relational database.
    data is the dict from parse_email()
    report_text is the markdown string generated by the LLM
    report_type is one of "daily_reports", "weekly_reports", "monthly_reports"
    """
    # Insert or replace the daily report
    supabase.table(report_type).upsert({
        "date": data["date"],
        "n_logs": data["n_logs"],
        "average_match": data["average_match"],
        "complete_misses": data["complete_misses"],
        "complete_misses_rate": data["complete_misses_rate"],
        "report_text": report_text
    }).execute()
    print(f"✅ Saved report for {data['date']}")

def fetch_past_week_reports(today):
    """Fetch daily reports from the last 7 days."""
    one_week_ago = today - timedelta(days=7)
    res = supabase.table("daily_reports").select("*") \
        .gte("date", one_week_ago.isoformat()) \
        .lte("date", today.isoformat()) \
        .order("date", ascending=True).execute()
    return res.data  # TODO return also the date and the week range...???

def fetch_past_month_reports(today):
    """Fetch weekly reports from the last calendar month."""
    first_day_this_month = today.replace(day=1)

    res = supabase.table("weekly_reports").select("*") \
        .gte("date", first_day_this_month.isoformat()) \
        .lte("date", today.isoformat()) \
        .order("date", ascending=True).execute()
    return res.data