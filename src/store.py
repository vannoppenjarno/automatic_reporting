from supabase import create_client
from dotenv import load_dotenv
from datetime import timedelta
import chromadb
import sqlite3
import hashlib
import os

load_dotenv()

REPORTS_DIR = os.getenv("REPORTS_DIR")
DB_PATH = os.getenv("DB_PATH")
DB_NAME = os.getenv("DB_NAME")

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# CLIENT = chromadb.PersistentClient(path=DB_PATH)
CHROMA_KEY = os.getenv("CHROMA_KEY")
client = chromadb.CloudClient(
  api_key=CHROMA_KEY,
  tenant='2b20ce89-8d98-4de1-8644-51260fb2110c',
  database='Test'
)
COLLECTION = client.get_or_create_collection(name="interactions")

def stable_id(date: str, time: str, question: str) -> str:
    q_hash = hashlib.md5(question.encode("utf-8")).hexdigest()
    return f"{date}_{time}_{q_hash}"

def save_report(report_text, date, folder=REPORTS_DIR):
    """Save the daily report as a markdown file in the reports folder."""
    # Ensure reports folder exists
    os.makedirs(folder, exist_ok=True)

    # Normalize date string (e.g., '2025-09-16' → '2025-09-16.json')
    filename = f"{date}.json"

    # Full path
    filepath = os.path.join(folder, filename)

    with open(filepath, "w", encoding="utf-8") as f:
        f.write(report_text)

    print(f"✅ Report saved: {filepath}")

def init_db(db_path: str = DB_PATH + DB_NAME):
    """Create database schema if it does not exist."""
    conn = sqlite3.connect(db_path)
    cur = conn.cursor()

    # Table for individual interactions
    cur.execute("""
    CREATE TABLE IF NOT EXISTS interactions (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        date TEXT,
        question TEXT,
        answer TEXT,
        match_score REAL,
        time TEXT,
        embedding BLOB
    )
    """)

    # Unique index to prevent duplicate questions at the same date & time
    cur.execute("""
    CREATE UNIQUE INDEX IF NOT EXISTS idx_interactions_unique
    ON interactions(date, time, question)
    """)

    # Regular indexes for faster queries
    cur.execute("CREATE INDEX IF NOT EXISTS idx_interactions_date ON interactions(date)")
    cur.execute("CREATE INDEX IF NOT EXISTS idx_interactions_match_score ON interactions(match_score)")

    # Table for daily reports
    cur.execute("""
    CREATE TABLE IF NOT EXISTS daily_reports (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        date TEXT UNIQUE,
        n_logs INTEGER,
        average_match REAL,
        complete_misses INTEGER,
        complete_misses_rate REAL,
        report_text TEXT
    )
    """)

    # Table for weekly reports
    cur.execute("""
    CREATE TABLE IF NOT EXISTS weekly_reports (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        date TEXT UNIQUE,
        n_logs INTEGER,
        average_match REAL,
        complete_misses INTEGER,
        complete_misses_rate REAL,
        report_text TEXT
    )""")

    # Table for monthly reports
    cur.execute("""
    CREATE TABLE IF NOT EXISTS monthly_reports (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        date TEXT UNIQUE,
        n_logs INTEGER,
        average_match REAL,
        complete_misses INTEGER,
        complete_misses_rate REAL,
        report_text TEXT
    )
    """)

    conn.commit()
    conn.close()

def update_db_interactions(data):
    """Insert interactions into Supabase and Chroma Cloud."""
    date = data["date"]
    for log in data["logs"]:
        Q = log["question"]
        A = log["answer"]
        S = log["match_score"]
        T = log["time"]
        E = log["embedding"]

        # 1️⃣ Supabase insert (Relational DB)
        try:
            supabase.table("interactions").insert({
                "date": date,
                "time": T,
                "question": Q,
                "answer": A,
                "match_score": S,
                "embedding": bytes(E)  # convert numpy array to bytes
            }).execute()
        except Exception as e:
            print(f"⚠️ Duplicate or error: {Q[:30]}... {e}")

        # 2️⃣ Chroma Cloud (Vector DB)
        COLLECTION.add(
            documents=[Q],
            metadatas=[{
                "answer": A,
                "match_score": S,
                "date": date,
                "time": T
            }],
            ids=[stable_id(date, T, Q)],
            embeddings=[E]
            )
        
    print(f"✅ Stored {len(data['logs'])} questions in both Relational and Vector DB for {data['date']}")
    return

def update_db_reports(data, report_text, report_type="daily_reports"):
    """
    Save the generated daily report into the Relational database.
    data is the dict from parse_email()
    report_text is the markdown string generated by the LLM
    report_type is one of "daily_reports", "weekly_reports", "monthly_reports"
    """
    # Insert or replace the daily report
    supabase.table(report_type).upsert({
        "date": data["date"],
        "n_logs": data["n_logs"],
        "average_match": data["average_match"],
        "complete_misses": data["complete_misses"],
        "complete_misses_rate": data["complete_misses_rate"],
        "report_text": report_text
    }).execute()
    print(f"✅ Saved report for {data['date']}")

def fetch_past_week_reports(today):
    """Fetch daily reports from the last 7 days."""
    one_week_ago = today - timedelta(days=7)
    res = supabase.table("daily_reports").select("*") \
        .gte("date", one_week_ago.isoformat()) \
        .lte("date", today.isoformat()) \
        .order("date", ascending=True).execute()
    return res.data  # TODO return also the date and the week range...???

def fetch_past_month_reports(today):
    """Fetch weekly reports from the last calendar month."""
    first_day_this_month = today.replace(day=1)

    res = supabase.table("weekly_reports").select("*") \
        .gte("date", first_day_this_month.isoformat()) \
        .lte("date", today.isoformat()) \
        .order("date", ascending=True).execute()
    return res.data