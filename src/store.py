from supabase import create_client
# import chromadb, time
import hashlib
import os

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# CHROMA_KEY = os.getenv("CHROMA_KEY")
# CHROMA_TENANT = os.getenv("CHROMA_TENANT")
# CHROMA_DATABASE = os.getenv("CHROMA_DATABASE")
# CHROMA_COLLECTION_NAME = os.getenv("CHROMA_COLLECTION_NAME")

# for attempt in range(3):
#     try:
#         client = chromadb.CloudClient(
#         api_key=CHROMA_KEY,
#         tenant=CHROMA_TENANT,
#         database=CHROMA_DATABASE
#         )
#         COLLECTION = client.get_or_create_collection(name=CHROMA_COLLECTION_NAME)
#         break
#     except Exception as e:
#         print(f"Connection attempt {attempt+1} failed: {e}")
#         time.sleep(3)
# else:
#     raise RuntimeError("Failed to connect to Chroma Cloud after 3 retries.")

def stable_id(date: str, time: str, question: str) -> str:
    q_hash = hashlib.md5(question.encode("utf-8")).hexdigest()
    return f"{date}_{time}_{q_hash}"

def update_db_interactions(data):
    """Insert interactions into Supabase and Chroma Cloud."""
    date = data["date"]
    for log in data["logs"]:
        Q = log["question"]
        A = log["answer"]
        S = log["match_score"]
        T = log["time"]
        E = log["embedding"]

        # 1️⃣ Supabase insert (Relational DB)
        try:
            supabase.table("interactions").insert({
                "date": date,
                "time": T,
                "question": Q,
                "answer": A,
                "match_score": S,
                "embedding": E
            }).execute()
        except Exception as e:
            print(f"⚠️ Duplicate or error: {Q[:30]}... {e}")

        # 2️⃣ Chroma Cloud (Vector DB)
        # COLLECTION.add(
        #     documents=[Q],
        #     metadatas=[{
        #         "answer": A,
        #         "match_score": S,
        #         "date": date,
        #         "time": T
        #     }],
        #     ids=[stable_id(date, T, Q)],
        #     embeddings=[E]
        #     )
        
    # print(f"✅ Stored {len(data['logs'])} questions in both Relational and Vector DB for {data['date']}")
    print(f"✅ Stored {len(data['logs'])} questions in Relational DB for {data['date']}")
    return

def update_db_reports(data, report_text, report_type="Daily"):
    """
    Save the generated daily report into the Relational database.
    data is the dict from parse_email()
    report_text is the markdown string generated by the LLM
    report_type is one of "Daily", "Weekly", "Monthly"
    """
    # Insert or replace the report
    try:
        supabase.table(report_type).upsert({
            "date": data["date"],
            "n_logs": data["n_logs"],
            "average_match": data["average_match"],
            "complete_misses": data["complete_misses"],
            "complete_misses_rate": data["complete_misses_rate"],
            "report_text": report_text
        }).execute()
    except Exception as e:
        print(f"⚠️ Error saving report for {data['date']}: {e}")
        return
    print(f"✅ Saved report for {data['date']}")
    return

def fetch_questions(date_range):
    """
    Fetch questions from Supabase within a date range and compute summary statistics.
    Returns a dict identical in structure to parse_email() output:
    {
        "date": "<end_date>",
        "n_logs": int,
        "average_match": float,
        "complete_misses": int,
        "complete_misses_rate": float,
        "logs": [ {question, answer, match_score, time, embedding}, ... ]
    }
    """
    start_date, end_date = date_range

    # Convert to ISO format if needed
    if hasattr(start_date, "isoformat"):
        start_date = start_date.isoformat()
    if hasattr(end_date, "isoformat"):
        end_date = end_date.isoformat()

    try:
        # Fetch all interactions in range
        res = (
            supabase.table("interactions")
            .select("date, time, question, answer, match_score, embedding")
            .gte("date", start_date)
            .lte("date", end_date)
            .order("date", desc=False)
            .order("time", desc=False)
            .execute()
        )
        rows = res.data or []

        logs = []
        accumulated_match = 0.0
        complete_misses = 0

        for r in rows:
            q = r["question"]
            a = r["answer"]
            s = float(r.get("match_score", 0))
            t = r["time"]
            e = r.get("embedding", None)

            logs.append({
                "question": q,
                "answer": a,
                "match_score": s,
                "time": t,
                "embedding": e
            })

            accumulated_match += s
            if s == 0:
                complete_misses += 1

        n_logs = len(logs)
        avg_match = round(accumulated_match / n_logs, 2) if n_logs > 0 else 0
        complete_misses_rate = round((complete_misses / n_logs) * 100, 2) if n_logs > 0 else 0

        data = {
            "date": start_date,
            "n_logs": n_logs,
            "average_match": avg_match,
            "complete_misses": complete_misses,
            "complete_misses_rate": complete_misses_rate,
            "logs": logs
        }

        print(f"✅ Fetched {n_logs} questions ({start_date} → {end_date}) | Avg match: {avg_match}% | Misses: {complete_misses}")
        return data

    except Exception as e:
        print(f"⚠️ Error fetching questions from {start_date} → {end_date}: {e}")
        return {
            "date": start_date,
            "n_logs": 0,
            "average_match": 0,
            "complete_misses": 0,
            "complete_misses_rate": 0,
            "logs": []
        }